{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranieri-unimi/faces-malchiodi-2022/blob/main/faces.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgQoZYRAWKCK"
      },
      "source": [
        "### run once"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark"
      ],
      "metadata": {
        "id": "2uhwYor6WVuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mH3gc9TWKCM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KAGGLE_USERNAME\"] = 'ranieriunimi'\n",
        "os.environ[\"KAGGLE_KEY\"] = str(hex(232307088475198570779809482024044346960))[2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngXoXE4KWKCN"
      },
      "outputs": [],
      "source": [
        "ref = 'bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows'\n",
        "!mkdir datasets\n",
        "!kaggle datasets download $ref --unzip -p ./datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kywqu11WKCQ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('all');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ukraine"
      ],
      "metadata": {
        "id": "oAR7LusA5ZOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_SIZE = 1024"
      ],
      "metadata": {
        "id": "c1FjJN8F6ejg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqGrV6XdWKCI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "import findspark"
      ],
      "metadata": {
        "id": "NrOTb9eEWSpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoA9_EouWKCO"
      },
      "outputs": [],
      "source": [
        "# load dataset \n",
        "filename = r\"./datasets/UkraineCombinedTweetsDeduped20220227-131611.csv.gzip\"\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "df = pd.read_csv(filename, compression='gzip', index_col=0, encoding='utf-8', quoting=csv.QUOTE_ALL)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data cleaning"
      ],
      "metadata": {
        "id": "i3RapMVFenPc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIzYGa4CWKCQ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "eng_sw = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_frlylt5WKCP"
      },
      "outputs": [],
      "source": [
        "df = df[df.language == 'en'].text.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if SAMPLE_SIZE:\n",
        "    df = random.sample(df, SAMPLE_SIZE)"
      ],
      "metadata": {
        "id": "FCWDtxA26udo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i9KWOCjWKCR"
      },
      "outputs": [],
      "source": [
        "# https://www.pluralsight.com/guides/building-a-twitter-sentiment-analysis-in-python\n",
        "\n",
        "def preprocess_tweet_text(tweet):\n",
        "    index, tweet = tweet\n",
        "\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "    # cleanings üßπ\n",
        "\n",
        "    # urls\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
        "\n",
        "    # # @ and #\n",
        "    # tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
        "\n",
        "    # punctuations\n",
        "    # tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "    tweet = tweet.translate(str.maketrans(string.punctuation+'‚Ä¶‚Äô‚Äù‚Äú', ' '*(len(string.punctuation)+4)))  # puntctuation to spaces\n",
        "\n",
        "\n",
        "    # tweet_tokens = word_tokenize(tweet)\n",
        "\n",
        "    # # emojitter\n",
        "    # wrds = [e for word in tweet_tokens for e in re.findall(r\"(\\w+|[^\\w ]+)\", word)]\n",
        "    # # TODO split also emoji-goups\n",
        "\n",
        "    # # stopwords\n",
        "    # filtered_words = [w for w in wrds if not w in eng_sw]\n",
        "    \n",
        "    # # stemmatize\n",
        "    # ps = PorterStemmer()\n",
        "    # stemmed_words = [ps.stem(w) for w in filtered_words]\n",
        "\n",
        "    # # lemmatize\n",
        "    # lemmatizer = WordNetLemmatizer()\n",
        "    # lemma_words = [str(lemmatizer.lemmatize(w, pos='a')) for w in stemmed_words]\n",
        "  \n",
        "    # return (index, lemma_words)\n",
        "\n",
        "    tweet = ' '.join(tweet.strip().split())\n",
        "\n",
        "    return (index, tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABxWwFYYWKCP"
      },
      "outputs": [],
      "source": [
        "#lang_hist = {l:df[df.language == l].size for l in df.language.unique()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REaNAuF2WKCS"
      },
      "source": [
        "## hadoop instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gMPGu1YWKCS"
      },
      "outputs": [],
      "source": [
        "# import findspark\n",
        "# findspark.init(\"spark-3.1.1-bin-hadoop3.2\") # SPARK_HOME\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize(enumerate(df))"
      ],
      "metadata": {
        "id": "_Kll5uQHB5hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.take(5)"
      ],
      "metadata": {
        "id": "ymR79ZEiwFsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataPipe = rdd.map(preprocess_tweet_text)"
      ],
      "metadata": {
        "id": "8TJTiM7aB-vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataPipe.take(5)"
      ],
      "metadata": {
        "id": "Wk0H5gR9vXIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# indexing = dataset.flatMap(lambda _, v : [(e,e) for e in v]).reduceByKey(lambda k, v : k)"
      ],
      "metadata": {
        "id": "qweNpiR__zu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### foos"
      ],
      "metadata": {
        "id": "5KLgYMu3DFUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_sub(sub, lst) : return all(e in lst for e in sub)\n",
        "def add(a, b) : return a+b\n",
        "def splat(t): return tuple(sorted(list(j for i in t for j in (i if isinstance(i, tuple) else (i,)))))\n",
        "def doubled(t): return len(set(t)) == len(t)"
      ],
      "metadata": {
        "id": "5yG0xwKXOhzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nen_rdd = sc.parallelize(df)"
      ],
      "metadata": {
        "id": "n4z4knWFC7bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# elemListPipe = nen_rdd.flatMap(lambda word_list : word_list).distinct()\n",
        "# elem_index = sc.parallelize(enumerate(elemListPipe.collect()))"
      ],
      "metadata": {
        "id": "yye2c9EaDIx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### shingles"
      ],
      "metadata": {
        "id": "aEl3HXq_t3BN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shin_len = 4"
      ],
      "metadata": {
        "id": "Kd6TFHOnzwH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataPipe.flatMap(lambda x : [(x[-1][i:i+shin_len], x[0]) for i in range(0, len(x[-1])-shin_len)]).distinct()"
      ],
      "metadata": {
        "id": "DiKs3Kllt2MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "HSEED = 42\n",
        "random.seed(HSEED)"
      ],
      "metadata": {
        "id": "Xt9LF_xgFubB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HBYTES = 2\n",
        "HMAX = 2**(8*HBYTES)\n",
        "SBOX = list(range(HMAX))\n",
        "random.shuffle(SBOX)"
      ],
      "metadata": {
        "id": "Es-WvH84FIpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def phash(message):\n",
        "  hash = len(message) % HMAX\n",
        "  for i in message:\n",
        "    hash = SBOX[(hash + ord(i)) % HMAX]\n",
        "  return hash.to_bytes(HBYTES, 'big')"
      ],
      "metadata": {
        "id": "pWXaw2VL8l_3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "e2cd697ea1a65fb50dbc24e0f8485a2ca518dbc7d7a135f655e94e4f59b8539e"
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "ukraine.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}